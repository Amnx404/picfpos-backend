{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "import os\n",
    "#import Image\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "classes = ['3d','c0','c1','c10','c2','c3','c4','c5','c6','c7','c8','c9','e1','e2','ent','ex','exb','i','m','meet','mod','r','sew','sv','tech','wm1','wm2']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def load_resnet50_finetuned():\n",
    "    # Load a pre-trained ResNet50 model\n",
    "    model_resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Unfreeze some of the layers for fine-tuning\n",
    "    for name, child in model_resnet50.named_children():\n",
    "        if name in ['layer3', 'layer4']:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Modify the final layer for  len(dataset.classes) classes\n",
    "    num_ftrs = model_resnet50.fc.in_features\n",
    "    model_resnet50.fc = nn.Linear(num_ftrs,  len(classes))\n",
    "\n",
    "    model_resnet50 = model_resnet50.to(device)\n",
    "\n",
    "    # Define loss function and optimizer for ResNet50\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_resnet50.parameters()), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return model_resnet50, optimizer, criterion\n",
    "\n",
    "\n",
    "def load_resnet101():\n",
    "    # Load a pre-trained ResNet101 model\n",
    "    model_resnet101 = models.resnet101(pretrained=True)\n",
    "\n",
    "    # Modify the final layer for  len(dataset.classes) classes\n",
    "    num_ftrs = model_resnet101.fc.in_features\n",
    "    model_resnet101.fc = nn.Linear(num_ftrs,  len(classes))\n",
    "\n",
    "    model_resnet101 = model_resnet101.to(device)\n",
    "\n",
    "    # Define loss function and optimizer for ResNet101\n",
    "    optimizer = optim.Adam(model_resnet101.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return model_resnet101, optimizer, criterion\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "chosen_model = load_resnet101()[0]\n",
    "#load from state dict\n",
    "chosen_model.load_state_dict(torch.load('./models/FC_Res101_simple/epoch_8.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_inference(image):\n",
    "    # Function to preprocess the image\n",
    "    def preprocess_image(image_path):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        #directly take in image file in variable\n",
    "        # image = Image.open(image)\n",
    "        \n",
    "        image = Image.open(image_path)\n",
    "        image = Image.open(image_path).convert('RGB')  # Convert to RGB\n",
    "        image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "        return image\n",
    "\n",
    "    def predict_image(model, image_path, class_names):\n",
    "        image = preprocess_image(image_path)\n",
    "        image = image.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predicted_class = class_names[predicted[0].item()]\n",
    "\n",
    "        return predicted_class\n",
    "\n",
    "    class_names = classes\n",
    "    preprocess_image(image)\n",
    "    p = predict_image(chosen_model, image, class_names)\n",
    "    # print(p)\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_inference('./frames/2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENT', 'C10', 'C4', 'C3', 'C2', 'C1', 'C5', 'C0', 'WM2', 'WM1']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check:\n",
    "# Completing the edges with distances and labels\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Define the edges with their distances and labels\n",
    "edges_with_distances_and_labels = {\n",
    "    ('WM1', 'WM2'): {'distance': 9, 'label': 90},\n",
    "    ('WM2', 'WM1'): {'distance': 9, 'label': 270},\n",
    "    ('EX', 'C0'): {'distance': 6, 'label': 90},\n",
    "    ('C0', 'EX'): {'distance': 6, 'label': 270},\n",
    "    ('C0', 'C5'): {'distance': 5, 'label': 90},\n",
    "    ('C5', 'C0'): {'distance': 5, 'label': 270},\n",
    "    ('C1', 'C2'): {'distance': 7, 'label': 90},\n",
    "    ('C2', 'C1'): {'distance': 7, 'label': 270},\n",
    "    ('C5', 'SV'): {'distance': 3, 'label': 90},\n",
    "    ('SV', 'C5'): {'distance': 3, 'label': 270},\n",
    "    ('E1', 'E2'): {'distance': 7, 'label': 90},\n",
    "    ('E2', 'E1'): {'distance': 7, 'label': 270},\n",
    "    ('E2', 'M'): {'distance': 6, 'label': 90},\n",
    "    ('M', 'E2'): {'distance': 6, 'label': 270},\n",
    "    ('I', 'C6'): {'distance': 3, 'label': 90},\n",
    "    ('C6', 'I'): {'distance': 3, 'label': 270},\n",
    "    ('C2', 'C3'): {'distance': 4, 'label': 90},\n",
    "    ('C3', 'C2'): {'distance': 4, 'label': 270},\n",
    "    ('C3', 'C4'): {'distance': 5, 'label': 90},\n",
    "    ('C4', 'C3'): {'distance': 5, 'label': 270},\n",
    "    ('EXB', 'C9'): {'distance': 8, 'label': 90},\n",
    "    ('C9', 'EXB'): {'distance': 8, 'label': 270},\n",
    "    ('C10', 'ENT'): {'distance': 4, 'label': 90},\n",
    "    ('ENT', 'C10'): {'distance': 4, 'label': 270},\n",
    "    ('C7', 'MEET'): {'distance': 4, 'label': 90},\n",
    "    ('MEET', 'C7'): {'distance': 4, 'label': 270},\n",
    "    ('R', '3D'): {'distance': 5, 'label': 90},\n",
    "    ('3D', 'R'): {'distance': 5, 'label': 270},\n",
    "    ('C0', 'WM2'): {'distance': 8, 'label': 0},\n",
    "    ('WM2', 'C0'): {'distance': 8, 'label': 180},\n",
    "    ('MOD', 'C1'): {'distance': 8, 'label': 0},\n",
    "    ('C1', 'MOD'): {'distance': 8, 'label': 180},\n",
    "    ('C5', 'E1'): {'distance': 8, 'label': 0},\n",
    "    ('E1', 'C5'): {'distance': 8, 'label': 180},\n",
    "    ('C1', 'C5'): {'distance': 4, 'label': 0},\n",
    "    ('C5', 'C1'): {'distance': 4, 'label': 180},\n",
    "    ('C8', 'C2'): {'distance': 8, 'label': 0},\n",
    "    ('C2', 'C8'): {'distance': 8, 'label': 180},\n",
    "    ('C2', 'C6'): {'distance': 4, 'label': 0},\n",
    "    ('C6', 'C2'): {'distance': 4, 'label': 180},\n",
    "    ('C6', 'E2'): {'distance': 8, 'label': 0},\n",
    "    ('E2', 'C6'): {'distance': 8, 'label': 180},\n",
    "    ('C3', 'TEC'): {'distance': 4, 'label': 0},\n",
    "    ('TEC', 'C3'): {'distance': 4, 'label': 180},\n",
    "    ('R', 'C3'): {'distance': 5, 'label': 0},\n",
    "    ('C3', 'R'): {'distance': 5, 'label': 180},\n",
    "    ('C4', 'SEW'): {'distance': 4, 'label': 0},\n",
    "    ('SEW', 'C4'): {'distance': 4, 'label': 180},\n",
    "    ('3D', 'C4'): {'distance': 5, 'label': 0},\n",
    "    ('C4', '3D'): {'distance': 5, 'label': 180},\n",
    "    ('EXB', '3D'): {'distance': 4, 'label': 0},\n",
    "    ('3D', 'EXB'): {'distance': 4, 'label': 180},\n",
    "    ('C7', 'C10'): {'distance': 5, 'label': 0},\n",
    "    ('C10', 'C7'): {'distance': 5, 'label': 180},\n",
    "    ('C9', 'C7'): {'distance': 4, 'label': 0},\n",
    "    ('C7', 'C9'): {'distance': 4, 'label': 180},\n",
    "    ('C4', 'C10'): {'distance': 3, 'label': 90},\n",
    "    ('C10', 'C4'): {'distance': 3, 'label': 270}\n",
    "}\n",
    "\n",
    "\n",
    "# Create a directed graph\n",
    "G_directional_distances = nx.DiGraph()\n",
    "\n",
    "# Add edges to the graph with distances as weights\n",
    "for edge, attributes in edges_with_distances_and_labels.items():\n",
    "    G_directional_distances.add_edge(edge[0], edge[1], weight=attributes['distance'], label=attributes['label'])\n",
    "\n",
    "# Function to find the shortest path with Dijkstra's algorithm considering distance\n",
    "def find_shortest_path_dijkstra(graph, start_node, end_node):\n",
    "    try:\n",
    "        # Compute the shortest path using Dijkstra's algorithm\n",
    "        path = nx.dijkstra_path(graph, source=start_node, target=end_node, weight='weight')\n",
    "        for i in range(len(path)-1):\n",
    "            edge = (path[i], path[i+1])\n",
    "            direction = graph.edges[edge]['label']\n",
    "            distance = graph.edges[edge]['weight']\n",
    "        return path\n",
    "    except (nx.NetworkXNoPath, KeyError):\n",
    "        # Return an empty list if no path exists or if the nodes are not in the graph\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def find_shortest_path_bfs(graph, start_node, end_node):\n",
    "\n",
    "    try:\n",
    "        path = find_shortest_path_dijkstra(graph, start_node, end_node)\n",
    "        return path\n",
    "    except (nx.NetworkXNoPath, KeyError):\n",
    "        # Return an empty list if no path exists or if the nodes are not in the graph\n",
    "        return []\n",
    "\n",
    "# Example usage of the function\n",
    "# Find the shortest path from 'CAF' to 'ENT'\n",
    "path_caf_to_ent = find_shortest_path_bfs(G_directional_distances, 'ENT', 'WM1')\n",
    "path_caf_to_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 45\n",
    "def get_direction_and_path(graph, start_node, end_node, alpha_rotation):\n",
    "    \"\"\"\n",
    "    Get the direction and path from start_node to end_node in a graph.\n",
    "\n",
    "    :param graph: NetworkX graph\n",
    "    :param start_node: starting node\n",
    "    :param end_node: ending node\n",
    "    :param alpha_rotation: current alpha rotation with respect to true North\n",
    "    :return: tuple (path, directions)\n",
    "    \"\"\"\n",
    "\n",
    "    def calculate_direction(edge, current_alpha):\n",
    "        \"\"\"\n",
    "        Calculate the direction to face for a given edge, adjusting for the current alpha rotation.\n",
    "        The graph directions are 45 degrees to the right of true north and are clockwise.\n",
    "        \"\"\"\n",
    "        graph_direction = graph.edges[edge]['label']\n",
    "        adjusted_direction = (graph_direction - current_alpha + 360 + shift) % 360\n",
    "        return adjusted_direction\n",
    "\n",
    "    path = find_shortest_path_bfs(graph, start_node, end_node)\n",
    "    directions = []\n",
    "\n",
    "\n",
    "    edge = (path[0], path[1])\n",
    "    direction = calculate_direction(edge, alpha_rotation)\n",
    "\n",
    "\n",
    "    return path, direction\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: ['C1', 'C5']\n",
      "Relative Direction: B\n"
     ]
    }
   ],
   "source": [
    "def get_relative_direction(angle):\n",
    "    \"\"\"\n",
    "    Convert an angle to a relative direction (F, L, B, R).\n",
    "\n",
    "    :param angle: Angle in degrees, where 0 degrees is the direction you're facing\n",
    "    :return: A string representing the relative direction ('F', 'L', 'B', 'R')\n",
    "    \"\"\"\n",
    "    if 315 <= angle < 360 or 0 <= angle < 45:\n",
    "        return 'F'  # Forward\n",
    "    elif 45 <= angle < 135:\n",
    "        return 'R'  # Right\n",
    "    elif 135 <= angle < 225:\n",
    "        return 'B'  # Backward\n",
    "    elif 225 <= angle < 315:\n",
    "        return 'L'  # Left\n",
    "    else:\n",
    "        return 'Unknown'  # For unexpected cases\n",
    "\n",
    "# Example usage\n",
    "path, angle = get_direction_and_path(G_directional_distances, 'C1', 'C5', 200)  # Replace 0 with your alpha rotation\n",
    "print(\"Path:\", path)\n",
    "\n",
    "\n",
    "direction = get_relative_direction(angle)\n",
    "print(\"Relative Direction:\", direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
